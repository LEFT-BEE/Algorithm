{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Softmax_Regression_Pratice.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOaehgn8tS6Qy4lguWmBEN3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LEFT-BEE/KMU-Algorithm/blob/master/%EB%8D%B0%EC%9D%B4%ED%84%B0%EA%B3%BC%ED%95%99/Softmax_Regression_Pratice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNZox1zP4QiL"
      },
      "source": [
        "## Softmax Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9zNGaUC4BW0"
      },
      "source": [
        "import torch\n",
        "x_train = torch.FloatTensor([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5],\n",
        " [1,2,5,6], [1,6,6,6], [1,7,7,7] ])\n",
        "y_train = torch.FloatTensor([ [0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0],\n",
        " [1,0,0], [1,0,0] ])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5urnPHx4NdF"
      },
      "source": [
        "# W,b 초기화 \n",
        "#optimizer 생성\n",
        "W = torch.zeros(4, 3, requires_grad=True)\n",
        "b = torch.zeros(1, 3, requires_grad=True)\n",
        "optimizer = torch.optim.Adam([W,b], lr=0.1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9lL5HT54jTF"
      },
      "source": [
        "$H(x) = S(x^Tw +b)$\n",
        "\n",
        "$Cost(w,b) = \\frac{1}{n}\\sum^m_{i=1} C(y_i , H(x_i))$\n",
        "\n",
        "$C(y,\\hat{y}) = -\\sum^d_{j=1} y_j log(\\hat{y}_j) $ \n",
        "-> 크로스 엔트로피 j는 배치데이터의 수 y는 실제 데이터 hat(y)는 예측값"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU0_7_zI4b7P",
        "outputId": "39b329b9-2dc6-4685-ef28-966170f8a4cf"
      },
      "source": [
        "# 반복횟수 설정, 가설 및 비용 설정\n",
        "for epoch in range(3001):\n",
        " #hypothesis = torch.softmax(torch.mm(x_train, W)+b, dim=1)\n",
        " #cost = -torch.mean(torch.sum(y_train * torch.log(hypothesis), dim=1))\n",
        "\n",
        " hypothesis = (torch.mm(x_train, W)+b).softmax(dim=1)\n",
        " #softmax 는 sogmoid에 비해 유연하게 학습이 가능함\n",
        " cost = -(y_train * torch.log(hypothesis)).sum(dim=1).mean()\n",
        " #tensor연산의 *은 일반적인 곱셈이 아닌 행렬곱을 의미한다\n",
        " optimizer.zero_grad()\n",
        " cost.backward()\n",
        " optimizer.step()\n",
        "\n",
        " if epoch % 300 == 0:\n",
        "  print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 0.002582\n",
            "epoch: 300, cost: 0.002092\n",
            "epoch: 600, cost: 0.001712\n",
            "epoch: 900, cost: 0.001412\n",
            "epoch: 1200, cost: 0.001173\n",
            "epoch: 1500, cost: 0.000979\n",
            "epoch: 1800, cost: 0.000822\n",
            "epoch: 2100, cost: 0.000692\n",
            "epoch: 2400, cost: 0.000585\n",
            "epoch: 2700, cost: 0.000496\n",
            "epoch: 3000, cost: 0.000421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zb8kOmr6y3D",
        "outputId": "026f8018-313f-40d0-df02-bf9e6c91bffd"
      },
      "source": [
        "#infer \n",
        "W.requires_grad_(False)\n",
        "b.requires_grad_(False)\n",
        "x_test = torch.FloatTensor([[1,11,10,9], [1,3,4,3], [1,1,0,1]])\n",
        "test_all = torch.softmax(torch.mm(x_test, W)+b, dim=1)\n",
        "print(test_all)\n",
        "print(torch.argmax(test_all, dim=1))\n",
        "#리스트값중 가장 큰 값을 출력 -> softmax의 값이 높을 수록 해당 레이블이 정답 이기에 스코어함수라고 부르기도함"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000e+00, 2.8566e-25, 0.0000e+00],\n",
            "        [7.2722e-03, 7.5765e-01, 2.3508e-01],\n",
            "        [2.9427e-44, 6.3150e-15, 1.0000e+00]])\n",
            "tensor([0, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ksM_Uhf8iYo"
      },
      "source": [
        "## 조금더 깔끔하게 Softmax\n",
        "\n",
        "1.레이블을 간단하게 입력받고 싶음 \n",
        "\n",
        "ex) [1,0,0], [0,1,0], [0,0,1] -> 0, 1, 2\n",
        "\n",
        "2.가설함수와 활성화 함수를 매번 구현하기 어려움\n",
        "\n",
        "ex) hypothesis = torch.softmax(torch.mm(x_train, W)+b, dim=1)\n",
        "\n",
        "cost = -torch.mean(torch.sum(y_train * torch.log(hypothesis), dim=1)) 와 같은 함수들"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx-R6ji_8GQ4"
      },
      "source": [
        "# pytoch의 functional 라이브러리를 F로 사용하겠다 선언\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "x_train = torch.FloatTensor([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5],\n",
        " [1,2,5,6], [1,6,6,6], [1,7,7,7] ])\n",
        "#문제 1번 해결 64비트의 부호 있는 정수는 torch.LongTensor를 사용한다.\n",
        "y_train = torch.LongTensor([2,2,2,1,1,1,0,0])\n",
        "\n",
        "model = nn.Linear(4,3)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=1)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZffroMP_bs7",
        "outputId": "39afdd1c-8fa7-4740-9121-3f8b160196ad"
      },
      "source": [
        "for epoch in range(3001): \n",
        " z = model(x_train)\n",
        " #생성한 모델에 학습데이터 입력\n",
        " cost = F.cross_entropy(z, y_train)\n",
        " #F.cross_entropy는 softamx와 cross_entropy가 합쳐진 것!\n",
        " optimizer.zero_grad()\n",
        " cost.backward()\n",
        " optimizer.step()\n",
        "\n",
        " if epoch % 300 == 0:\n",
        "  print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 2.762645\n",
            "epoch: 300, cost: 0.028284\n",
            "epoch: 600, cost: 0.011228\n",
            "epoch: 900, cost: 0.006065\n",
            "epoch: 1200, cost: 0.003805\n",
            "epoch: 1500, cost: 0.002602\n",
            "epoch: 1800, cost: 0.001880\n",
            "epoch: 2100, cost: 0.001412\n",
            "epoch: 2400, cost: 0.001089\n",
            "epoch: 2700, cost: 0.000858\n",
            "epoch: 3000, cost: 0.000687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5UxpiQMBvlx"
      },
      "source": [
        "## Softmax Regression with Sklearn\n",
        "\n",
        "sklearn 에서는 logisticRegression이 다중분류 문제에서 Softmax regression으로 구현됨\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkujaPiiBeZO",
        "outputId": "44b937a5-c09b-4228-b3c9-c2843a933231"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "x_train = np.array([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5],\n",
        " [1,2,5,6], [1,6,6,6], [1,7,7,7] ])\n",
        "y_train = np.array([ 2, 2, 2, 1, 1, 1, 0, 0 ])\n",
        "\n",
        "logistic = LogisticRegression() #모델생성 \n",
        "logistic.fit(x_train, y_train) #train_data와 label을 넣어줌 for문으로 학습하는 과정이 fit함수하나로 요약됨 \n",
        "\n",
        "\n",
        "pred = logistic.predict([[1,11,10,9], [1,3,4,3], [1,1,0,1]]) #추론단계\n",
        "print(pred)\n",
        "#입력에대한 출력값이 잘 나온다."
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 2 2]\n"
          ]
        }
      ]
    }
  ]
}